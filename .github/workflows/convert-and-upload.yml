name: Convert SafeTensors to GGUF and Upload

on:
  workflow_dispatch:  # manual trigger only

jobs:
  convert:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install Git LFS
        run: |
          sudo apt-get update
          sudo apt-get install -y git-lfs
          git lfs install

      - name: Clone model repo
        run: |
          git clone https://huggingface.co/CWClabs/CWC-Mistral-Nemo-12B-V2-q4_k_m 
          ls -lh CWC-Mistral-Nemo-12B-V2-q4_k_m

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Build llama.cpp
        run: |
          sudo apt-get install -y build-essential cmake
          git clone https://github.com/ggerganov/llama.cpp 
          cd llama.cpp
          cmake -B build -DLLAMA_NATIVE=OFF -DCMAKE_BUILD_TYPE=Release
          cmake --build build -j$(nproc) --target llama-quantize
          echo "$PWD/build/bin" >> $GITHUB_PATH

      - name: Convert to FP16 GGUF
        run: |
          cd llama.cpp
          python convert.py ../CWC-Mistral-Nemo-12B-V2-q4_k_m --outfile ../model.f16.gguf

      - name: Quantize to Q4_K_M
        run: |
          llama-quantize model.f16.gguf model.q4_K_M.gguf q4_K_M

      - name: Install Hugging Face CLI
        run: pip install huggingface_hub

      - name: Upload GGUF to Hugging Face
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          huggingface-cli upload Berigny/cwc-mistral-nemo-12b-v2-q4_k_m-gguf \
            model.q4_K_M.gguf \
            model.q4_K_M.gguf \
            --token $HF_TOKEN